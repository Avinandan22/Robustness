---
layout: ../layouts/Layout.astro
title: Keeping up with dynamic attackers Certifying robustness to adaptive online data poisoning
description: Simple project page template for your research paper, built with Astro and Tailwind CSS
favicon: favicon.svg
thumbnail: screenshot-light.png
---

import Header from "../components/Header.astro";
import Video from "../components/Video.astro";
import HighlightedSection from "../components/HighlightedSection.astro";
import SmallCaps from "../components/SmallCaps.astro";
import Figure from "../components/Figure.astro";
import Image from "../components/Image.astro";
import TwoColumns from "../components/TwoColumns.astro";
import YouTubeVideo from "../components/YouTubeVideo.astro";
import LaTeX from "../components/LaTeX.astro";

import { ImageComparison } from "../components/ImageComparison.tsx";

import outside from "../assets/outside.mp4";
import transformer from "../assets/transformer.webp";
import Splat from "../components/Splat.tsx"
import dogsDiffc from "../assets/dogs-diffc.png"
import dogsTrue from "../assets/dogs-true.png"
import Figure3 from "../assets/Figure3.avif"
import schematic from "../assets/schematic.png"
import table1 from "../assets/table1.webp"
import mean from "../assets/mean.avif"


import CodeBlock from "../components/CodeBlock.astro";
import Table from "../components/Table.astro";
export const components = {pre: CodeBlock, table: Table}

<Header
  title={frontmatter.title}
  authors={[
    {
      name: "Avinandan Bose",
      institution: "University of Washington",
      notes: [],
    },
    {
      name: "Laurent Lessard",
      institution: "Northeastern University",
      notes: [],
    },
    {
      name: "Maryam Fazel",
      institution: "University of Washington",
      notes: [],
    },
    {
      name: "Krishnamurthy (Dj) Dvijotham",
      institution: "ServiceNow Research",
      notes: [],
    },
  ]}
  conference="AISTATS 2025"
  links={[
    {
      name: "Paper",
      url: "https://arxiv.org/pdf/2502.16737",
      icon: "ri:file-pdf-2-line",
    },
    {
      name: "Code",
      url: "https://github.com/Avinandan22/Certified-Robustness",
      icon: "ri:github-line",
    },
    {
      name: "Blog",
      url: "https://www.servicenow.com/blogs/2025/robustness-against-dynamic-data-poisoning",
      icon: "ri:article-line", 
    },
  ]} 
/>


## Introduction

Artificial Intelligence (AI) relies on massive, uncurated datasets, where verifying data quality is often impractical. As AI models increasingly incorporate input from untrusted users, they become more vulnerable to carefully orchestrated attacks that can implant malicious behaviors into AI. Adversarial data poisoning poses significant cybersecurity risks, ranging from life-threatening misdiagnoses to market disruptions. As illustrated in Table 1, these attacks can vary depending on the stage at which they occur, during training or deployment. 

<Figure>
  <Image slot="figure" source={table1} altText="Table 1: Cybersecurity risks of adversarial data poisoning" />
  <span slot="caption">Table 1: Cybersecurity risks of adversarial data poisoning.</span>
</Figure>

Most prior research focuses on certifying robustness of AI training algorithms against static adversaries that modify a portion of the offline dataset before the training algorithm is applied (as shown in Figure 1). 

Dynamically optimized attack strategies enhance the effectiveness of adversarial poisoning. Dynamic attackers can observe the trained algorithm in real time and adapt their poisoning tactics to the evolving behavior of the ML model. This was first demonstrated in [this paper](https://arxiv.org/abs/1808.08994), and [subsequent](http://proceedings.mlr.press/v120/zhang20b.html) [work](https://proceedings.mlr.press/v97/liu19e/liu19e.pdf) has expanded these initial findings in multiple directions.

Dynamic attacks is particularly true for models that are continuously or periodically updated, such as foundation models that undergo periodic fine-tuning or reinforcement learning from human feedback (RLHF).

We introduce a new way to protect ML systems from dynamic data poisoning attacks. We developed a framework that helps us measure how much an attack could affect the model, and we use these measurements to create learning algorithms that can better handle such attacks.

## The Problem Setup
We assume that our learning algorithm is trying to estimate a set of parameters <LaTeX formula='\theta' inline={true}/> (like the parameters of a neural network). We study online learning algorithms of the form 

<LaTeX formula='\theta^\prime \gets F(\theta, z)'/> 

where  <LaTeX formula='z' inline={true} /> denotes the current datapoint or batch of datapoints. <LaTeX formula='F' inline={true}/> represents any learning algorithm, and includes as special cases variants of gradient descent, reinforcement learning and evolutionary algorithms.

Some of the data points may be poisoned by an adversary who wants to mislead the learning algorithm towards parameters that optimize an adversarial objective  <LaTeX formula='\ell_{\text{adv}}(\theta)' inline={true}/>
which could represent the prediction error of the model (for an adversary simply targeting reducing the performance of the learning algorithm) or more targeted objectives (like the model having a favorable view of a certain political party). At each step, the algorithm might receive poisoned data with a certain probability. This is controlled by a parameter <LaTeX formula='\epsilon' inline={true}/>, which determines what fraction of data points are poisoned. Thus we have

<LaTeX formula='z = z_{\text{adv}} \text{ with probability} \epsilon, z \sim \mathbb{P}_{\text{data}} \text{ with probability } 1-\epsilon'/>

where <LaTeX formula='\mathbb{P}_{\text{data}}' inline={true} /> is the true (non-poisoned) data distribution.

<Figure>
  <Image slot="figure" source={schematic} altText="Schematic diagrams highlighting the differences between static and dynamic data poisoning" />
  <span slot="caption">Figure 1: Schematic diagrams highlighting the differences between static and dynamic data poisoning.</span>
</Figure>


## General Framework to Compute Certificate


The learning process forms a Markov chain with a transition kernel

<LaTeX formula='\Pi_{z_{\text{adv}}}(\theta^\prime|\theta)= \delta(F(\theta, z))\text{ where } z \sim \epsilon\delta(z^{\text{adv}}) + (1-\epsilon)\mathbb{P}_{\text{data}}' />

where <LaTeX formula='\delta' inline={true}/> denotes the Dirac delta distribution with all probability mass concentrated on a single point. The adversary's goal is to maximize

<LaTeX formula='\lim_{T \to \infty} \frac{1}{T}\mathbb{E}\left[\sum_{t=0}^T \ell_{\text{adv}}(\theta_t)\right]' />

Our main contribution is a robustness certificate, that computes an upper bound on the objective  This is a tool that helps us understand how much damage an adversarial attack can do to a model. Using this certificate, we can design algorithms that are more resistant to attacks (see Figure 2). We calculate how different choices of poisoned data influence the parameter estimates. This gives us a "measure" of the model's robustness. The certificate allows us to predict and limit the damage done by these attacks.

In particular, we show that , we have that

<HighlightedSection>

### Theorem
For any function <LaTeX formula='\lambda(\theta)' inline={true}/>, the worst case objective achieved by the adversary is at most

> > <LaTeX formula='\sup_{\theta, z_{\text{adv}}}\mathbb{E}_{\theta^\prime \sim \Pi_{z_\text{adv}}(\theta^\prime|\theta)}[\lambda(\theta^\prime))]- \lambda(\theta) +\ell_{\text{adv}}(\theta)' /> - (*)

</HighlightedSection>

## Derivation of the Certificate

To see why the above result is true, let's assume that <LaTeX formula='\mu' inline={true}/> denotes the stationary distribution for the Markov Chain induced by <LaTeX formula='\Pi' inline={true}/>. We are interested in upper bounding

<LaTeX formula='\mathbb{E}_{\theta \sim \mu}[\ell_{\text{adv}}(\theta)]'/>

We have that <LaTeX formula='\mathbb{E}_{\theta \sim \mu}[\Pi(\theta_^\prime|\theta)]=\mu(\theta^\prime)'/> since <LaTeX formula='\mu' inline={true}/> is stationary (note that we drop the subscript <LaTeX formula='\Pi_{z_{\text{adv}}}'/> for brevity). Thus, we can add and subtract the expectation of an arbitrarily chosen function <LaTeX formula='\mu' inline={true}/> from the objective to obtain

<LaTeX formula='\mathbb{E}_{\theta \sim \mu}[\ell_{\text{adv}}(\theta)-\lambda(\theta) + \lambda(\theta)]=\mathbb{E}_{\theta \sim \mu}[\ell_{\text{adv}}(\theta)-\lambda(\theta)] + \mathbb{E}_{\theta^\prime \sim \mu}[\mathbb{E}_{\theta \sim \Pi(\theta|\theta^\prime)}[\lambda(\theta)]]'/>

Noting that in the final term we can relabel <LaTeX formula='\theta \to \theta^\prime, \theta^\prime \to \theta' inline={true}/>, the above expression can be seen as an expectation over $\theta \sim \mu$:

<LaTeX formula='\mathbb{E}_{\theta \sim \mu}[\ell_{\text{adv}}(\theta)-\lambda(\theta) + \mathbb{E}_{\theta^\prime \sim \Pi_{z_{\text{adv}}}(\theta^\prime|\theta)}[\mu(\theta^\prime)]]'/>

Since  <LaTeX formula='\mu' inline={true}/> and <LaTeX formula=' \Pi_{z_{\text{adv}}}(\theta^\prime|\theta)' inline={true}/> are probability measures, no matter what strategy the dynamic adversary chooses, they cannot increase the adversarial objective beyond

<LaTeX formula='\sup_{\thete, z_{\text{adv}}}[\ell_{\text{adv}}(\theta)-\lambda(\theta) + \mathbb{E}_{\theta^\prime \sim \Pi_{z_{\text{adv}}}(\theta^\prime|\theta)}[\mu(\theta^\prime)]]'/>

showing that our certificate is indeed an upper bound on the adversarial objective (*)

##Meta-learning a robust learning algorithm

In order to make the above certificate practically useful, we adopt a meta-learning framework we aim to design a learning algorithm that performs well across a range of distributions. We optimize a combination of two objectives:



The first component focuses on the algorithm's ability to perform effectively without an adversary, by ensuring it converges to a stable set of parameters that results in low expected loss. The second component provides an upper bound on the potential worst-case loss the algorithm might face when an adversary is involved.


The key here is to find a defense that works well on average, even when faced with new, unseen data distributions.
<Figure>
  <Image slot="figure" source={Figure3} altText="Schematic of robustness certificate." />
  <span slot="caption">Figure 2: Schematic of robustness certificate.</span>
</Figure>



 <HighlightedSection>

## Proof of concept experiments on mean estimation


We tested our defense on a set of 50 Gaussian distributions, where the defense was trained on 10 randomly chosen distributions, and we tested how well it performed on held out distributions under varying levels of data poisoning.

The results (see Figure 3) demonstrate that our defense significantly improves performance compared to training without any defense. We varied the learning rates and the fraction of samples corrupted by the dynamic adversary.

Based on our results, our algorithm performs much better at estimating the mean, even when a significant fraction of the data is poisoned.


<Figure>
  <Image slot="figure" source={mean} altText="Figure 4: Test performance (mean squared error between true and estimated means)" />
  <span slot="caption">Figure 3: Test performance (mean squared error between true and estimated means).</span>
</Figure>

</HighlightedSection>
## Conclusions

Although we illustrated our framework using mean estimations and binary classification, its applications extend far beyond this simple example. Our defense strategies are particularly valuable for real-world AI systems, from binary classifiers making critical yes/no decisions (such as fraud detection in financial transactions) to multiclass systems categorizing data into multiple categories (such as image recognition or document topic classification).

More significantly, the framework can protect reward models in reinforcement learning systems, where corrupted feedback could fundamentally alter AI behavior and learning trajectory.

Our robustness certificates provide AI practitioners with tools to quantify and mitigate vulnerabilities to dynamic poisoning attacks. As AI systems become increasingly integrated in critical applications, these defenses offer a mathematical foundation for building more reliable and trustworthy AI systems.

## BibTeX citation

```bibtex
@article{bose2025keeping,
  title={Keeping up with dynamic attackers: Certifying robustness to adaptive online data poisoning},
  author={Bose, Avinandan and Lessard, Laurent and Fazel, Maryam and Dvijotham, Krishnamurthy Dj},
  journal={arXiv preprint arXiv:2502.16737},
  year={2025}
}
```
